{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.switch_backend('agg')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_real, normalize=None):\n",
    "    \"\"\"Compute confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_pred (list[int] | np.ndarray[int]): Prediction labels.\n",
    "        y_real (list[int] | np.ndarray[int]): Ground truth labels.\n",
    "        normalize (str | None): Normalizes confusion matrix over the true\n",
    "            (rows), predicted (columns) conditions or all the population.\n",
    "            If None, confusion matrix will not be normalized. Options are\n",
    "            \"true\", \"pred\", \"all\", None. Default: None.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Confusion matrix.\n",
    "    \"\"\"\n",
    "    if normalize not in ['true', 'pred', 'all', None]:\n",
    "        raise ValueError(\"normalize must be one of {'true', 'pred', \"\n",
    "                         \"'all', None}\")\n",
    "\n",
    "    if isinstance(y_pred, list):\n",
    "        y_pred = np.array(y_pred)\n",
    "    if not isinstance(y_pred, np.ndarray):\n",
    "        raise TypeError(\n",
    "            f'y_pred must be list or np.ndarray, but got {type(y_pred)}')\n",
    "    if not y_pred.dtype == np.int64:\n",
    "        raise TypeError(\n",
    "            f'y_pred dtype must be np.int64, but got {y_pred.dtype}')\n",
    "\n",
    "    if isinstance(y_real, list):\n",
    "        y_real = np.array(y_real)\n",
    "    if not isinstance(y_real, np.ndarray):\n",
    "        raise TypeError(\n",
    "            f'y_real must be list or np.ndarray, but got {type(y_real)}')\n",
    "    if not y_real.dtype == np.int64:\n",
    "        raise TypeError(\n",
    "            f'y_real dtype must be np.int64, but got {y_real.dtype}')\n",
    "\n",
    "    label_set = np.unique(np.concatenate((y_pred, y_real)))\n",
    "    num_labels = len(label_set)\n",
    "    label_map = {label: i for i, label in enumerate(label_set)}\n",
    "    confusion_mat = np.zeros((num_labels, num_labels), dtype=np.int64)\n",
    "    for rlabel, plabel in zip(y_real, y_pred):\n",
    "        index_real = label_map[rlabel]\n",
    "        index_pred = label_map[plabel]\n",
    "        confusion_mat[index_real][index_pred] += 1\n",
    "\n",
    "    with np.errstate(all='ignore'):\n",
    "        if normalize == 'true':\n",
    "            confusion_mat = (\n",
    "                confusion_mat / confusion_mat.sum(axis=1, keepdims=True))\n",
    "        elif normalize == 'pred':\n",
    "            confusion_mat = (\n",
    "                confusion_mat / confusion_mat.sum(axis=0, keepdims=True))\n",
    "        elif normalize == 'all':\n",
    "            confusion_mat = (confusion_mat / confusion_mat.sum())\n",
    "        confusion_mat = np.nan_to_num(confusion_mat)\n",
    "\n",
    "    return confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, figsize=(10,10), save_path=None, normalize=False, colorbar=False):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "        class_names (array, shape = [n]): String names of the integer classes\n",
    "        figsize: figure pannel size\n",
    "        save_path: the path to save the confusion matrix as an image\n",
    "        normalize: normalized or not\n",
    "    Returns:\n",
    "        figure: matplotlib figure of the confusion matrix\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=figsize)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "#     plt.title(\"Confusion matrix\")\n",
    "    if colorbar:\n",
    "        plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=90)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        if normalize:\n",
    "            if cm[i,j] == 0:\n",
    "                number = '0'\n",
    "            else:\n",
    "                number = '{:0.1f}'.format(100*cm[i, j])\n",
    "        else:\n",
    "            number = cm[i,j]        \n",
    "        plt.text(j, i, number, horizontalalignment=\"center\", color=color)\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up path for thh reference (baseline) model prediction and its ground truth\n",
    "# The reason why we have two different ground truth is that the orders could be different across different machines\n",
    "ref_prediction_path = '../../work_dirs/r2plus1d_r34_video_3d_8x8x1_900e_ucf101_rgb_20percent_vidssl/results.pkl'\n",
    "ref_gt_labels_path = '../../work_dirs/r2plus1d_r34_video_8x8x1_180e_ucf101_rgb_fixmatch_20percent_vidssl/gt_labels.npy'\n",
    "\n",
    "# class names \n",
    "classnames_path = '../../work_dirs/r2plus1d_r34_video_8x8x1_180e_ucf101_rgb_fixmatch_20percent_vidssl/classnames.npy'\n",
    "\n",
    "# set up the path for the compared (proposed) model predictions, and its ground truth\n",
    "comp_prediction_path = '../../work_dirs/r2plus1d_r34_video_8x8x1_360e_ucf101_rgb_all_20percent_vidssl/results.pkl' # final model\n",
    "comp_gt_labels_path = '../../work_dirs/r2plus1d_r34_video_3d_8x8x1_900e_ucf101_rgb_20percent_vidssl/gt_labels_new.npy'\n",
    "# comp_prediction_path = '../../work_dirs/r2plus1d_r34_video_8x8x1_180e_ucf101_rgb_taugment_20percent_vidssl/results.pkl' # temp aug all only\n",
    "# comp_gt_labels_path = '../../work_dirs/r2plus1d_r34_video_8x8x1_180e_ucf101_rgb_fixmatch_20percent_vidssl/gt_labels.npy'\n",
    "# comp_prediction_path = '../../work_dirs/r2plus1d_r34_video_8x8x1_180e_ucf101_rgb_actorcutmix_20percent_vidssl/results.pkl' # ActorCutMix only\n",
    "# comp_gt_labels_path = '../../work_dirs/r2plus1d_r34_video_3d_8x8x1_900e_ucf101_rgb_20percent_vidssl/gt_labels_new.npy'\n",
    "\n",
    "# set up the output bar chart path for both worse classes and better classes\n",
    "ref_output_bar_chart_path = '../../work_dirs/confmat_baseline.jpg'\n",
    "comp_output_bar_chart_path = '../../work_dirs/confmat_final.jpg'\n",
    "# comp_output_bar_chart_path = '../../work_dirs/confmat_tempaug_only.jpg'\n",
    "# comp_output_bar_chart_path = '../../work_dirs/confmat_actorcutmix_only.jpg'\n",
    "\n",
    "# canvas size\n",
    "figsize = (50,50)\n",
    "\n",
    "# normalize or not\n",
    "# normalize = 'true'\n",
    "normalize = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "ref_preds = np.load(ref_prediction_path, allow_pickle=True)\n",
    "ref_preds = [entry.argmax() for entry in ref_preds]\n",
    "comp_preds = np.load(comp_prediction_path, allow_pickle=True)\n",
    "comp_preds = [entry.argmax() for entry in comp_preds]\n",
    "ref_gt_labels = np.load(ref_gt_labels_path)\n",
    "comp_gt_labels = np.array(np.load(comp_gt_labels_path, allow_pickle=True))\n",
    "classnames = np.load(classnames_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref pred top-1 accuracy: 38.91%\n",
      "Compared pred accuracy: 56.73%\n"
     ]
    }
   ],
   "source": [
    "# compute ref model confmat, top-1 accuracy\n",
    "confmat_ref = confusion_matrix(ref_preds, ref_gt_labels, normalize=normalize)\n",
    "if normalize == 'true':\n",
    "    ref_overall_acc = np.sum(np.diag(confmat_ref))\n",
    "    print('Ref pred top-1 accuracy: {:.2f}%'.format(ref_overall_acc))\n",
    "else:\n",
    "    ref_overall_acc = np.sum(np.diag(confmat_ref))/float(len(ref_gt_labels))\n",
    "    print('Ref pred top-1 accuracy: {:.2f}%'.format(100*ref_overall_acc))\n",
    "\n",
    "# compute compared model confmat, top-1 accuracy\n",
    "confmat_comp = confusion_matrix(comp_preds, comp_gt_labels, normalize=normalize)\n",
    "if normalize == 'true':\n",
    "    comp_overall_acc = np.sum(np.diag(confmat_comp))\n",
    "    print('Compared pred accuracy: {:.2f}%'.format(comp_overall_acc))\n",
    "else:\n",
    "    comp_overall_acc = np.sum(np.diag(confmat_comp))/float(len(comp_gt_labels))\n",
    "    print('Compared pred accuracy: {:.2f}%'.format(100*comp_overall_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confmat_ref, classnames, figsize, ref_output_bar_chart_path, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confmat_comp, classnames, figsize, comp_output_bar_chart_path, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute the confusion matrices by the ascending order of the baseline class accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc_ascending_order = np.argsort(np.diag(confmat_ref))\n",
    "confmat_ref_perm = confmat_ref[baseline_acc_ascending_order]\n",
    "confmat_ref_perm = confmat_ref_perm[:,baseline_acc_ascending_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confmat_ref_perm, classnames[baseline_acc_ascending_order], figsize, ref_output_bar_chart_path, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat_comp_perm = confmat_comp[baseline_acc_ascending_order]\n",
    "confmat_comp_perm = confmat_comp_perm[:,baseline_acc_ascending_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(confmat_comp_perm, classnames[baseline_acc_ascending_order], figsize, comp_output_bar_chart_path, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
